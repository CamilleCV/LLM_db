{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\OneDrive\\Documentos\\Projetos\\2025-06 LLM com acesso ao db\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['equipamentos', 'ordem_tecnico', 'ordens_manutencao', 'tecnicos'])\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SQLDatabase\n",
    "from llama_index.core.objects import SQLTableNodeMapping\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import SQLTableSchema\n",
    "\n",
    "\n",
    "# Importar a secret key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"secret_key\")\n",
    "\n",
    "# Iniciar o db\n",
    "url = 'manutencao_industrial.db'           \n",
    "engine = create_engine(f'sqlite:///{url}') \n",
    "\n",
    "# Objeto para coletar as informações da eng\n",
    "metadata_obj = MetaData()  \n",
    "metadata_obj.reflect(engine)\n",
    "print(metadata_obj.tables.keys())\n",
    "\n",
    "# Iniciando o modelo\n",
    "modelo=\"llama-3.3-70b-versatile\"\n",
    "modelo_hf_emb=\"BAAI/bge-m3\"\n",
    "#Settings.llm = Groq(model=modelo, api_key = api_key)\n",
    "llm = Groq(model = modelo, api_key = api_key)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name = modelo_hf_emb)\n",
    "\n",
    "\n",
    "# Criando um mapeamento do db - SQLTableNodeMapping\n",
    "sql_database = SQLDatabase(engine)\n",
    "table_node_map = SQLTableNodeMapping(sql_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.query_pipeline import FnComponent\n",
    "from llama_index.core.objects import SQLTableSchema\n",
    "\n",
    "# Criar as tabelas como objeto para o Llama conseguir identificar\n",
    "table_squema_objs = []\n",
    "for nome_tabela in metadata_obj.tables.keys():\n",
    "    schema_info = sql_database.get_single_table_info(nome_tabela)\n",
    "    # Use o schema como descrição (pode adicionar mais, se quiser)\n",
    "    table_squema_objs.append(\n",
    "    SQLTableSchema(table_name=nome_tabela)\n",
    ")\n",
    "  \n",
    "\n",
    "# Criando o retriever\n",
    "obj_index = ObjectIndex.from_objects(table_squema_objs, table_node_map, VectorStoreIndex)\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "\n",
    "# Criando a descrição das tabelas\n",
    "def descricao_tabela(schema_tabelas: List[SQLTableSchema]):\n",
    "    descricao_str = []\n",
    "    for tabela_schema in schema_tabelas:\n",
    "        # Usa apenas o que está em context_str, que já foi salvo no retriever\n",
    "        descricao = tabela_schema.context_str or sql_database.get_single_table_info(tabela_schema.table_name)\n",
    "        descricao_str.append(descricao)\n",
    "    return '\\n\\n'.join(descricao_str)\n",
    "\n",
    "contexto_tabela = FnComponent(fn=descricao_tabela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1 - adaptado para PT\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "texto2sql = \"\"\"Dada uma pergunta em linguagem natural, crie uma consulta {dialect} sintaticamente correta para executar e, em seguida, verifique os resultados da consulta e retorne a resposta. Você pode ordenar os resultados por uma coluna relevante para retornar os exemplos mais informativos no banco de dados.\n",
    "\n",
    "Nunca consulte todas as colunas de uma tabela específica. Pergunte apenas por algumas colunas relevantes, de acordo com a pergunta.\n",
    "\n",
    "Preste atenção para usar apenas os nomes de colunas que você pode ver na descrição do esquema. Tenha cuidado para não consultar colunas que não existem. Preste atenção em qual coluna está em qual tabela. Além disso, qualifique os nomes das colunas com o nome da tabela quando necessário.\n",
    "\n",
    "Use o seguinte formato, cada um em uma linha:\n",
    "\n",
    "Pergunta: Pergunta aqui\n",
    "ConsultaSQL: Consulta SQL para executar\n",
    "ResultadoSQL: Resultado da ConsultaSQL\n",
    "Resposta: Resposta final aqui\n",
    "\n",
    "Use apenas as tabelas listadas abaixo.\n",
    "\n",
    "{schema}\n",
    "\n",
    "Pergunta: {pergunta_user}\n",
    "ConsultaSQL:\n",
    "\"\"\"\n",
    "\n",
    "prompt_1_template = PromptTemplate(texto2sql, dialect = engine.dialect.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_1_fn(pergunta_user: str, schema: str, conversation_history: str = \"\") -> str:\n",
    "    schema_com_hist = schema + \"\\nContexto anterior:\\n\" + conversation_history\n",
    "    # Aqui você chama o .format() no template\n",
    "    return prompt_1_template.format(pergunta_user=pergunta_user, schema=schema_com_hist)\n",
    "\n",
    "prompt_1_comp = FnComponent(fn=prompt_1_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a consulta_sql\n",
    "\n",
    "from llama_index.core.llms import ChatResponse\n",
    "\n",
    "def resposta_sql (resposta: ChatResponse) -> str:\n",
    "    '''O dado de entrada será ChatResponse\n",
    "    -> str informa que a saída deverá ser do tipo string'''\n",
    "    conteudo_resposta = resposta.message.content\n",
    "    \n",
    "    sql_consulta = conteudo_resposta.split(\"ConsultaSQL: \", 1)[-1].split(\"ResultadoSQL: \", 1)[0]\n",
    "    return sql_consulta.strip().strip('```').strip()\n",
    "# Strip para tirar textos em branco.\n",
    "# strip('```') para remover markdowns que possam vir no código da llm\n",
    "\n",
    "consulta_sql = FnComponent(fn=resposta_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para o resultado_sql\n",
    "\n",
    "from llama_index.core.retrievers import SQLRetriever\n",
    "\n",
    "resultado_sql = SQLRetriever(sql_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para o prompt 2:\n",
    "\n",
    "prompt_2_str = '''\n",
    "    Você é o \"Assitente de consulta de banco de dados da SiderTech Solutions\".\n",
    "    Dada a seguinte pergunta, a consulta SQL correspondente e o resultado SQL, responda à pergunta de modo agradável e objetivamente.\n",
    "    Evite iniciar conversas com cumprimentos e apresentações, como \"Olá\".\n",
    "\n",
    "    Pergunta: {pergunta_user}\n",
    "    Consulta SQL: {consulta}\n",
    "    Resultado SQL: {resultado}\n",
    "    Resposta:\n",
    "    '''\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    prompt_2_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_2_fn(pergunta_user: str, consulta: str, resultado: str, conversation_history: str = \"\") -> str:\n",
    "    texto_completo = prompt_2.format(\n",
    "        pergunta_user=pergunta_user,\n",
    "        consulta=consulta,\n",
    "        resultado=resultado\n",
    "    )\n",
    "    # Você pode acrescentar conversation_history se quiser usar contexto ali também\n",
    "    return texto_completo\n",
    "\n",
    "prompt_2_comp = FnComponent(fn=prompt_2_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar o contexto de meória\n",
    "\n",
    "from llama_index.core.query_pipeline import FnComponent\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def memory_component(input_text: str):\n",
    "    conversation_history.append(input_text)\n",
    "    # retorna todo o histórico concatenado (ou formatado)\n",
    "    return \"\\n\".join(conversation_history)\n",
    "\n",
    "memory = FnComponent(fn=memory_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camil\\AppData\\Local\\Temp\\ipykernel_17548\\1762823326.py:3: DeprecationWarning: Call to deprecated class QueryPipeline. (QueryPipeline has been deprecated and is not maintained.\n",
      "\n",
      "This implementation will be removed in a v0.13.0.\n",
      "\n",
      "It is recommended to switch to the Workflows API for a more flexible and powerful experience.\n",
      "\n",
      "See the docs for more information workflows: https://docs.llamaindex.ai/en/stable/understanding/workflows/)\n",
      "  qp = QueryPipeline(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline, InputComponent\n",
    "\n",
    "qp = QueryPipeline(\n",
    "    modules = {                               \n",
    "        'entrada': InputComponent(), \n",
    "        'memory': memory,         \n",
    "        'acesso_tabela': obj_retriever,   \n",
    "        'contexto_tabela': contexto_tabela,   \n",
    "        'prompt_1': prompt_1_comp,     # wrapper função aqui\n",
    "        'llm_1': llm,\n",
    "        'consulta_sql': consulta_sql,\n",
    "        'resultado_sql': resultado_sql,\n",
    "        'prompt_2': prompt_2_comp,     # wrapper função aqui\n",
    "        'llm_2': llm,\n",
    "    },\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp.add_chain(['entrada', 'memory', 'acesso_tabela', 'contexto_tabela'])\n",
    "qp.add_link('memory', 'prompt_1', dest_key='conversation_history')\n",
    "qp.add_link('memory', 'prompt_2', dest_key='conversation_history')\n",
    "qp.add_link('entrada', 'prompt_1', dest_key='pergunta_user')\n",
    "qp.add_link('contexto_tabela', 'prompt_1', dest_key='schema')\n",
    "qp.add_chain(['prompt_1', 'llm_1', 'consulta_sql', 'resultado_sql'])\n",
    "qp.add_link('entrada', 'prompt_2', dest_key='pergunta_user')\n",
    "qp.add_link('consulta_sql', 'prompt_2', dest_key='consulta')\n",
    "qp.add_link('resultado_sql', 'prompt_2', dest_key='resultado')\n",
    "qp.add_link('prompt_2', 'llm_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: O técnico que trabalhou na ordem 32 foi o \"Tecnico 3\".\n"
     ]
    }
   ],
   "source": [
    "print(str(qp.run(query='Qual técnico trabalhou na ordem 32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: A especialidade dele é elétrica.\n"
     ]
    }
   ],
   "source": [
    "print(str(qp.run(query='Qual a especialidade dele?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Os equipamentos que tiveram manutenção nos últimos 3 meses são:\n",
      "\n",
      "- Equipamento 9: 25/05/2025\n",
      "- Equipamento 2: 22/05/2025 e 22/04/2025\n",
      "- Equipamento 3: 07/06/2025, 04/06/2025 e 10/05/2025\n",
      "- Equipamento 8: 18/04/2025, 13/04/2025 e 31/03/2025\n",
      "- Equipamento 1: 17/05/2025 e 31/03/2025\n",
      "- Equipamento 4: 02/05/2025 e 02/04/2025\n",
      "- Equipamento 10: 23/05/2025, 18/05/2025, 03/05/2025 e 23/04/2025\n",
      "- Equipamento 5: 27/05/2025, 03/05/2025, 14/04/2025 e 11/06/2025\n",
      "- Equipamento 6: 20/06/2025 e 06/06/2025\n",
      "- Equipamento 7: 21/06/2025\n"
     ]
    }
   ],
   "source": [
    "print(str(qp.run(query='Quais os equipamentos que tiveram manutenção nos últimos 3 meses?')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: O nome do técnico que trabalhou em mais ordens de manutenção é Tecnico 3.\n"
     ]
    }
   ],
   "source": [
    "print(str(qp.run(query='Qual o nome do técnico que trabalhou em mais ordens de manutenção?')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
